
--Aug 27, 2021--

Proposed renamings:
	  condition_node.py -> conditions.py
	✓ knowledgebase -> memory
	  predicate_node -> literal
	✓ fact OK 
	Op -> Func
	var OK 


Thinking...
	Right now predicate nodes extract info from Vars when they are constructed...
	Ultimately it would be nice to be able to not just do something like "var op var", and also f(var, var,...) or f(var, f(var,var)) ect. So it seems that 
	it might make more sense for predicate nodes to just be Funcs, and for them
	to be characterized as alpha or not by the number of free variables. Using indexing on "==" is one special detail, and this would need to be turned on or off optionally... maybe that would go into "Context". But it seems that the actual index dictionary needs go into the linked instance, and the Func needs to be marked as indexable (Thought... are beta nodes indexable?)

	There is also the matter of evaluation. Vars and Funcs are both ultimately evaluatable given some binding. For Vars deref_attrs() needs to be executed. 

	Considerations:
		-It would be nice if Conditions were made up of some common thing... like cre.Evaluatable, where it can be executed and it's source is generated on the fly. 

	Evaluatables have in common:
		-A set of variables ('var_base_ptrs?')
		-A thing that happens when all of the variables are grounded 
		-A filter function? Maybe this is just hard coded?
		-A function for generating a string for itself.
		-Metadata?
		-Maybe some way of recasting itself to it's actual type if it is emitted into python. 
		-A way to copy itself... maybe just evaling it's string in python?

	When conditions are linked there is a representation problem. It would be best if the conditions could be rearranged freely. The cleanest thing might just be instead of having Pterms that are linked to Memories, there are normal Condition instances and linked instances? It looks like there is already a copy=True/False option for this. 

	It seems that in all cases a Pterm has enough in common with the proposed cre.Executable that maybe they should be merged. In principle an executable could be linked to a Memory in some contextual manner, that it might make sense for all the link stuff to just be in there. There is also the 'negated' property which could in principle be applied to non-binary values as a sort of existential NOT. Then again maybe Exectuable can just be standin for BasePredicateNodeType.


	There is some common structure like

	Conditions.link_mem(mem).get_matches() -> A list of Match Predicates
	Executable.link_mem(mem).get_matches() -> All Predicates/Tokens representing the execution of the executable.
	Var.link_mem(mem).get).get_matches() -> All values matching the var

	For Conditions some link data is maintained that keeps track of partial matches.
	For Var nothing needs to be maintained, the list of facts are simply returned
	For Func some link data is

	Maybe Predicates should be a specialization of Fact.  

	Predicates are just Pred(name, ...list of member predicates)
	Members should be Facts
	Or alternatively Pred(name,members=x)

	Atoms should also be facts and have .type_name and .value,

	But are Vars facts? Like Equals(boop1.A, boop2.A) -> 
	Vars are executables... maybe facts should also be executables?

	If I wanted a table of things like Equals(boop1.A, boop2.A), then this is more like a table of executables, in which case 

	Could define predicate types like Match=Pred("Match") or just Pred("Match")

	Maybe there is the grounded version of things i.e. Facts + Predicates and then the exectuable versions?

	So if I would like to do something like BOOP(x,y) >> BOOP(str(y),float(x))
	Then BOOP(x,y) both the LHS and RHS are executables.
	Or maybe BOOP(x,y) is really Var(BOOP,"z") & (x:=z.A) & (y:=z.B) >> BOOP(x,y)  
	or Var(BOOP,"z") >> BOOP(str(z.B), float(z.A))

	Is this invertable... I guess this is it's own inverse? But how do I find that?
	I guess the constructor BOOP would have an inversion BOOP(u,v) >> u & v and then str and float would be inverses of each other so BOOP(u,v)


	Var(str,"x") & Var(float,"y") & Var()





Example:
c = (
	Var(f8, "A") &
	Var(f8, "B") &
	(B+1 < A) & (A==1) & F(B)
)



--Aug 28, 2021--

CREBase(cre_type : u8)

What if there are
	Executables(base_var_ptrs : *Var[:], exec_func_addr : *Func):
		-Var
		-Func
		-Conditions
		-Rule(lhs: conditions, rhs : executable)
	Facts:
		-Normal Fact structs
		-Grounding (exec : cre.Executable, args : *Fact[:], value : Cre.Atom)
		-Predicate (name : str, args : *Fact[:])
		-Atom (type: str, value : any[float,str,list[any]])

Memories only consist of Facts. The flattened working memory would only consist of groundings. 

Executables all have base_var_ptrs, and an exec_func_addr



Example Rules:

#For all boops 'x' declare another boop with B=x.B+1
Var(BOOP, "x") >> BOOP(x.A,x.B+1)


#Define Predicate
Pred("GT")
(
	Var(BOOP,"x") & Var(BOOP,"y") & 
	(x.B <= (y.B + 1))
) >> (
	GT(x,y)
)


#Retract 
(
	Var(BOOP,"x") & Var(BOOP,"y") & 
	(x.B <= (y.B + 1))
) >> (
	NOT(x) 
)


Note: A var can be existentially NOT'ed, an executable can be negated
Potential qualifiers for an executable:
	TRUTHY(1), FALSEY(-1), VALUE(0)

If &/| are applied to put it in a conditions object then it should be TRUTHY 
If ~ is applied then it should be FALSEY
Otherwise it is VALUE 

When a FuncComp is &/| then it should be flattened in order to define an exec_func

Under what contexts does exec_func get applied?
	1. When the func is used as a literal term that we want to check a match against in which case we would want to just pass it pointers or Fact instances. In this case what do we do with the func's muted exceptions and check function?
	2. When we use it during get_matches and want to filter a partial match. In this case it needs to be wrapped in some loop, and it should really just return a boolean value. 
	3. When the Executable is used to compute groundings, and we want to use exec_func to produce the value of the grounding. 
	4. When the Executable shows up on the RHS of a Rule and we want to declare whatever comes out of the exec_func 

For cases 1 & 2 the exec can just be wrapped to be Truthy if it isn't already.


For cases 1 & 2 the type of the exec can be anything, in this context it probably makes sense to for exec_func to poop out an Atom if it doesn't already poop out a proper Fact. 

There is also the consideration that in this case we can have NOT(x) kinds of terms. How do we unify this with the Executable type? NOT only makes sense in the context of Vars and would in reality be wrapped in a Conditions object. So the Conditions object could just know that NOT(x) should be treated as a retraction at runtime. 

We need a way of declaring named facts using this shorthand. In principle this means that every fact should be able to take name as a kwarg, and have a name slot. 

There is also the runtime consideration that FactCtor(Var, Var,...) is actually an Op of sorts... Maybe I already handled this 


The first step to all of this is probably to make conditions work with Exectuables instead of with 

TODO:
[] Make a common cre.Executables supertype for Ops, Vars, and Conditions
[] Make Conditions work off of executables 
[] Make All facts capable of taking a name, except maybe not Atoms?


--Aug 29, 2021--

Going to simply reorganize Conditions so that it builds a DNF of Literals (i.e. an Op + other stuff). The main DNF will just be a List(List(Literal)), but at initialization it will be reorganized such that every conjunct 


--Sep 1, 2021--

We need to be able to reconstruct the python object for an Op instance. The most straight forward way would probably be to reapply it's __class__. This could either be stored in the cre_context() and retrieved by some unique repr of the Op, or somehow stored in the struct or datamodel of the Op. 


Op class:
	repr: Op[Add](signature=float64(float64,float64), members={})
	str: ...

Op:
	repr: Add(Add(a:float64, b:float64), c:float64) -> float64
	str : ((a + b) + c)
	expr_template : Add(Add({0},{1}),{2})
	shorthand_template : (({0} + {1}) + {2})


cases:
	1) Add(Var(float,"x"),Var(float,"y"))
	-Should produce a new instance of Add with the new Vars, and specialize Add if necessary.
	2) Add(Add(Var(float,"x"),Var(float,"y")), Var(float,"z"))
	-As above for the inner term, then make an OpComp that is flattened into an op
	3) Add(1,2)
	-Should specialize Add if needed and apply call()
	4) Add(1,Var(f8,"y"))
	-Should specialize Add if needed, make a new OpComp and then flatten it into an Op


To handle all of these we should make a resolve_return_type function that can take a Var, Op, or Constant 


TODO:
	[x] Fix bug with variables names not propogating w/ Op_comp
	[x] Make test cases for untypedOp

	[x] Make dereffing Vars need to be compiled
	[x] Implement repr() for Op


--Sep 3, 2021--

Next steps:
	[x] Overload <, >, <=, >=, ==, +, * , -, /
	[] Make sure conditions str/repr properly <-- probably can be the same except that repr should probably reveal that it is a member of the Conditions class and reveal any active mem_link.
	[] Define filter_op... the rest will probably just work...

---Sep 4, 2021---

Thoughts... I've gone back and forth between keeping alpha and beta bits seperate. It seems uncessary now that everything just uses literals/ops. It would make sense to just have the dnf be a list of lists,
and then at initialization things would be reordered into a structure with order:
	Dim 0: slot = argmax(left_var_num, right_var_num,...)
	Dim 1: slot = 
		(Indexed alphas < alphas) < (betas) < ...


distr_dnf:
Bins for each Var
Alphas need to be in their own bin
betas need to be in their own bin
After alphas are taken care of then they need to be passed to various betas.
In principle this means finding connected components among the betas
 1) ? each conjunct. <----
 2) ? each dnf?
And then picking some order that they should be checked.

Dictionaries are probably going to need to be the main data structure.
If on the left a dictionary is provided with keys [a_i,b_j,c_k]
and on the right a dictionary with keys [d_l,] is provided
then all [a,b,c] x [d,] should be checked for d < b and the output dictionary of passing values should be of [a,b,c,d]. Now how can we keep this result and only check diffed facts. In this instance we only care
about changes in b's and d's. The nodes being inputted into this
node should probably...

We need to keep track of all facts that have changed since we last updated,
so if the Conditions object's queue is just a bunch of FIDs that have changed then we need a way of querying these FIDs to see whether the current literal needs updating. This goes for the FIDs of facts that are only indirectly involved through dereferencing. This means we probably need to either:
	1) Not embed the dereferences in the op, and call deref_attrs 
	2) Keep some meta-data about the dereferences... which in practice means computing the intermediates anyway.

So then there really needs to be a 'depends' dictionary of FID->??

Maybe there only needs to be a depends dictionary and the rest is just vectors. For example a 2d vector is passed for [a,b,c] x [d,] and then [a,b,c,d] is a new vector. Along the way the 'depends' dictionary is filled with the FIDs for all d's and b's (plus FIDs for dereferences) as keys and the values are the indicies of the relevant values within the left, right and resultant vectors. In the case of retractions these vectors would need to be filled with zeros and the missing spots remembered.

Now on a retraction:
	an FID is put in the change queue. At the first node it is checked against the 'depends' (if the FID doesn't have an attr_id then the relevant one should be assumed), the output vector that the depends points to should be updated with the change. (Seems like we probably don't need to edit the input vectors after all, since they would have already been altered by the previous node). In the next nodes we do the same thing... checking the entire change queue against the 'depends' of the node... If a change in left makes left + right fail then we need to remove right from 'depends' as well.

	So probably in order for 'depends' to clean itself up properly it should be something like (output_index, ...FIDs to remove from self)... but maybe this doesn't work because other Left FID could still be needed by another output. Maybe 'depends' should be a nested dict(dict())???

In the original RETE everything works by propogation, the set of matches are literally passed along... does that not work here? What is missing...

The question is when something goes missing how to we remove only that thing from the output of a node... maybe we just search for it?

This is kind of the same problem I was having without even doing it this way, and the solution involved keeping around a consistency matrix... which probably would work fine here too... just the outputs would be different. 

Okay so looking back it seems that there is a truth_values matrix and a consistency matrix and the output is created by finding the true points in the truth table.

The consistency table is certainly convenient in some ways... Although it seems probably too intimately connect to the left/right fact vector sizes. So how can we make for a more minimal data structure...

At the end of the day the ONLY thing a node needs to achieve is consistency between it's inputs and output. So we can probably achieve something similar to the consistency matrix by just keeping two 'depends' dictionaries for the left and right... where I guess the keys are just the same as the inputs. This gets us back to passing around dictionaries. 

What if we did this. The Conditions object passes the items in the change queue along to their respective alpha nodes. The alpha nodes maintain their own change queues (which are maybe just cleared after each update) and when a downstream nodes goes to update, it rechecks the pairs in the union of left_change * right_all and left_all * right_changes. As a result of rechecking we might need to add or remove items ->  would alter the current node's change queue. 

Should node change queues consist of indicies or [...FIDs]? Indicies only work if we're using holey vectors.

To really update the outputs we either need to know where each output goes in an array or we need to just use a set/dictionary and add/del keys.

We'll need to use dictionaries. So then the change queue is just a dictionary of added things and removed things. For removed things we need to do the usual inner product and just take out any that are already in the output. For added things we do the same but only add them in if they evaluate to true.


-- Sep 6, 2021 --

Some thoughts... if x.B.B.A < y.B.A then one match to this depends on 10 (really 12) possible changes.

	-- Fact Retractions --
	-retraction of x
	-retraction of y

	-- Deref retractions / Modifications --
	-modification of x.B
	-retraction of x.B
	-modification of x.B.B
	-retraction of x.B.B
	-modification of y.B
	-retraction of y.B

	-- Head modifications --
	-modification of x.B.B.A
	-modification of y.B.A

	--- These two can be baked into retraction of x/y ---
	-x being retracted from the output of left
	-y being retracted from the output of right

Our 'depends' dictionary would need to have all of these possibilities
as keys and the values would need to be a list of outputs + the other keys that are depended upon.

If an output is invalidated then we need to remove all of its depends, but not any that are also part of another output. So we need some kind of refcounting solution.

So DependencyEntry would have:
	"count" : int 
	"outputs" : List() ???


This just seems like a ton of overhead... but maybe it's necessary?
The overhead for just one node might be more than conservatively rechecking everything.


Thought... how are things simplified if only considering fact retractions, declarations and head modifications?

What if every output kept track of it's own list of nodes + outputs that need to be retracted or rechecked. A node can edit this list, and signal a change. So if it is retracted all of its downstream dependencies are retracted. Is this that different from the 'depends' dict? A little bit because it reduces...

What if the fact itself keeps track of things that depend on it. So when a modification occurs it can signal a checked. 




'depends' = idrec -> array of pointers to OutputEntries()
	the output entries have other idrecs... we don't need to clean them
	out because the OutputEntry instance can just be marked as invalid

ConditionNode
	"inputs" : ListType(??),
    "output" : Vector(*OutputEntryType),
    "arg_indicies" : i8[::1],
    "literal" : LiteralType,
	"t_ids" : i8[::1],
	"depends" : DictType(i8, ListType(OutputEntryType))
	"fail_depends" : DictType(i8, i8)


OutputEntry
	"is_valid" : u1,
	"index" : i8,
	"f_ids" : u8[::1], #maybe u4[::1]
	"depends_idrecs" : u8[::1]

When update is called:
	1) look through the .mem's change_queue and check for
		(idrec ~ left_t_id | idrec ~ right_t_id)

		Also go through the the change queues of the inputs and use the equivalent idrecs for the node that would be created from the f_ids of the changed OutputEntries.

		Accumulate these changes... clobber declarations/retractions chronologically. Do on .mem first then the inputs.

		#if any of the .mem ones are in deref_fails then go to the OutputEntry associated with the input and add it to the appropriate left/right change set.


for each change:
	if(change ~ DEC):
		try left_changes * all_right
		try right_changes * all_left
		
		if(!passes because of deref):
			add to deref_fails

		if(passes):
			make the output entry
			add fids encountered into depends
			self.changed_entries.append((DEC, output_entry))

	else:
		if(idrec in depends):
			for output_entry in depends[idrec]:
				passes = ?passes(output_entry) if(change ~ MOD) else False
				if(not passes):
					if(output_entry.is_valid):
						outputs.remove(output_entry)
						output_entry.is_valid = False

					self.changed_entries.append((RET,output_entry))
					


Maybe it makes more sense to treat deref failures per input, so for example for the left inputs we try to deref everything and we see where it stops.  


So,
	left_deref_depends : idrec -> List(DerefRecord)
	right_deref_depends : idrec -> List(DerefRecord)

	when one of these changes we need to reattempt to find the fid of the head fact.

	so now once all of these get processed we should only have to worry about base_fids in the depends dict?

	From these we should be able to come up with a 
	'left_deref_records' and 'right_deref_records' and in the case that something changes (a deref chain now succeeds/fails/alters it's head) there is now an effective set of left_changes and right_changes which are just the base fids.


Psuedo code round 2:

ConditionNode
	"inputs" : ListType(??),
    "output" : Vector(*OutputEntryType),
    "arg_indicies" : i8[::1],
    "literal" : LiteralType,
	"deref_t_ids" : i8[::1],
	"depends" : DictType(i8, ListType(OutputEntryType))
	"deref_depends : List(DictType(i8, ListType(DerefRecord)))"
	"deref_consistencies" : List(vector(i8))"


OutputEntry
	"is_valid" : u1,
	"index" : i8,
	"f_ids" : u8[::1], #maybe u4[::1]
	"depends_idrecs" : u8[::1]

DerefRecord = {
	"is_valid" : u1,
	"consistency_index" : u1,
	"fids" : i8[::1],
}



for idrec in self.mem.change_queue:
	chg_typ, f_id = idrec
	for i in range(n_args):
		if(f_id in deref_depends[i]):
			
			if(chg_typ == DEC):

			else:
				deref_record = deref_depends[i][f_id]
				deref_consistencies[i].remove([deref_record.consistency_index]




Maybe it's better to just have a seperate DerefNode that lives inside each alpha/beta node. 

So DerefNode takes in the .output of all .inputs and produces a change_set that can be used by the alpha/beta node.

Cases:
	r1) retract(z)
	r2) retract(z.B)

	m1) modify(z.B)
	m2) modify(z.B.A)

	d) declare(z)

	//declare(z.B) <-This isn't possible. It would either be linked at instantiation or change with a subsequent modify.

Two things need to be up to date after an update:
	1) .outputs : Vector(*OutputEntry),
	2) .retracted_inds: Vector(i8) 
	3) .inds_of_diffs : Vector(i8) <- Indicies that need to be rechecked downstream


d:
	Only triggered by changes in .input.inds_of_diffs
	- .input.output.add(input.inds_of_diffs) needs to be in self.output
	- if .retracted_inds was nonempty then there should be one fewer
	- whatever index was used when added needs to be in .inds_of_diffs
	Should ignore other declarations

r1:
	triggered by changes in .input.inds_of_diffs?
	- 







--- Sept 7 ----

It dawned on me that there is potentially a lot of duplicate checking in normal RETE. For example if I am checking (b < c) then the matches of for instance a beta check (a < b) previous in the graph might produce multiple matches to the same b. We could alternative keep around data structures like: 

{b -> [a1,a2,a3]}
{a -> [b1,b2,b3]}

Probably only need one or the other, although the other one potentially double as a 'depends' dictionary at least for base declarations/retractions.

Reconstructing the match would then involve rebuilding the matches backwards through a set of nodes marked as resolution nodes... 




--- Sept 8 ---

The most straight-forward thing is probably going to be to define a function call_head() which executes the parts of the Op that are not DerefInstrs, and then leave the DerefInstrs parts to some generic piece like deref_attrs which will apply the dereferencing up to the address of the head value, and inject the fids of anything along the way into 'depends'

An alternative would be to break any DerefInstrs into seperate literals. I'm not sure that having them in seperate nodes would really be simpler though. It might help with nools translation... but would complicate the Conditions object with more bindable vars.







DerefRecord = {
    # Pointers to the Dict(i8,u1)s inside deref_depends
    "parent_ptrs" : i8[::1], 
    "arg_ind" : i8,
    "was_successful" : u1,
}


#Example Deref a.B.B.B.A
# dep_idrecs = [(1, MOD[a.B]), (2, RETRACT[a.B]),
#				(2, MOD[a.B.B]), (3, RETRACT[a.B.B]),
				(3, MOD[a.B.B.B]), (4, RETRACT[a.B.B.B]),
				(4, MOD[a.B.B.B.A])
					  ]




dict_i8_u1_type = DictType(i8,u1)

def invalidate_deref_rec(rec):
	r_ptr = _raw_ptr_from_struct(rec)
	for ptr in parent_ptrs:
		parent = _dict_from_ptr(dict_i8_u1_type, ptr)
		del parent[r_ptr]
		_decref_structref(rec)


def _make_deref_record_parent(idrec, r_ptr):
	p = self.deref_depends.get(idrec,None)
	if(p0 is None):	
		p0 = self.deref_depends[idrec] = Dict(i8,u1)
	p[r_ptr] = 1
	return _raw_ptr_from_struct(p0)


def validate_deref(k, f_id):
	'''Try to get the head_ptr of 'f_id' in input 'k'. Inject a DerefRecord regardless of the result '''
	t_id = self.t_ids[k]
	base_ptr = mem.facts[t_id][f_id]
	deref_f_ids, head_ptr = apply_deref(base_ptr,...)

	was_successful = (head_ptr != 0)

	parent_ptrs = np.empty(len(deref_idrecs), dtype=np.int64)
	r = DerefRecord(parent_ptrs, self.arg_inds[k], was_successful)
	r_ptr = _ptr_from_struct_incref(r)
	for i in range(len(deref_f_ids)):
		idrec0 = encode_idrec(?t_id, ?f_id, ??MODIFY)
		idrec1 = encode_idrec(?t_id, ?f_id, RETRACT)

		parent_ptrs[i*2] = _make_deref_record_parent(idrec0, r_ptr)
		parent_ptrs[i*2-1] = _make_deref_record_parent(idrec1, r_ptr)

	idrec0 = encode_idrec(?t_id, ?f_id, ??MODIFY)
	parent_ptrs[-1] = _make_deref_record_parent(idrec0, r_ptr)


	return head_ptr

	
def validate_head_or_retract(self,k,f_id, a_id):
	'''Update the head_ptr dictionaries by following the deref chains of DECLARE/MODIFY changes, and make retractions for an explicit RETRACT or a failure in the deref chain.''
	if(a_id0 != RETRACT):
		head_ptr = validate_deref(k, f_id0)
		if(head_ptr > 0): 
			self.inp_head_ptrs[k][f_id0] = head_ptr
			continue
		
	# At this point we are definitely RETRACT
	clear0 = self.outputs[k][a_id0]:
	for x in clear0:
		del self.outputs[1 if k else 0][x]
	del self.outputs[k][a_id]
	del self.inp_head_ptrs[k][f_id]



def filter_beta(self):
	arg_change_sets = List([Dict(i8,u1), Dict(i8,u1)])

	### 'relevant_global_diffs' is the set of self.mem.change_queue items relevant to intermediate derefs computed for this literal, and modification of the head attribute. Shouldn't happen frequently ###
	for idrec in self.relevant_global_diffs:
		if(idrec in self.deref_depends and len(self.deref_depends) > 0):
			deref_records = self.deref_depends[idrec]

			# Invalidate old DerefRecords
			for r in deref_records:
				r.invalidate()
				_, base_f_id, _ = decode_idrec(r.dep_idrecs[0])

				# Any change in the deref chain counts as a MODIFY
				arg_change_sets[r.arg_ind][base_f_id] = MODIFY


	### Make sure the arg_change_sets are up to date
	for i, inp in enumerate(self.inputs.change_set):
		arg_change_sets_i = arg_change_sets[i]
		if(len(arg_change_sets_i) > 0):
			 for f_id in inp.change_set:
			 	arg_change_sets_i.add(idrec)
		else:
			arg_change_sets[i] = inp.change_set


	### Update the head_ptr dictionaries by following the deref chains of DECLARE/MODIFY changes, and make retractions for an explicit RETRACT or a failure in the deref chain.
	for f_id0, a_id0 in arg_change_sets[0].items():
		validate_head_or_retract(self, f_id0, a_id0)

	for f_id1, a_id1 in arg_change_sets[1].items():
		validate_head_or_retract(self, f_id1, a_id1)

	### Check all pairs at this point we should only be dealing with DECLARE/MODIFY changes
	for f_id0, a_id0 in arg_change_sets[0].items():
		h_ptr0 = self.head_ptrs[0][f_id0]
		if(a_id0 != RETRACT):
	        for h_ptr1 in self.head_ptrs[1]
	            check_pair(h_ptr0, h_ptr1, a_id0)
			
    for f_id1, a_id1 in arg_change_sets[1].items():
    	if(a_id1 != RETRACT):
    		h_ptr1 = self.head_ptrs[1][f_id1]
	        for f_id0, h_ptr0 in self.head_ptrs[0].items()
	        	if(f_id0 not in arg_change_sets[0]):
	            	check_pair(h_ptr0, h_ptr1, a_id1)

def check_pair(self, h_ptr0, h_ptr1, chg_typ):
	passes = self.call_head_ptrs(h_ptr0, h_ptr1)

	# For any MODIFY type of change
	if(not passes and chg_typ != DECLARE):

	else:


See 'speed_experiments/bench_beta_data_structures.py' it seems like doing the full rete is probably worth it




 ---- Sept 9 ----

 We need to be able to do call() with and without check() we also need to be able to apply call from head_ptrs 

 So we could do:

 	@njit(head_sig)
 	call_head():
 		....

 	@njit(head_sig)
 	call_head_ptrs():
 		....
 		call_head(...)

 	@njit(head_sig)
 	call():
 		....
 		call_head(...)

Do I even need check()? It seems unecessary, the user can just raise their own exception.

----- Sept 13 -----

There is a common structure to all nodes alpha/beta with regard to dereferencing to heads. Every Op has a set of arguments and a resulting set of head ptrs, so the first step is always to:

1) Retract any relevant idrecs from depends from the global change set
2) Re-evaluate the changes and make sure that the head ptrs are up to date

There are potentially more head_ptrs than inputs since in principle we can do something like x.B.A < x.B.B.A

How should we deal with this?

Internally we need to be able to loop over the arg level and know which f_ids have changes queued. But we also need to be able to dereference any number of vars associated with the same arg. So The function validate_head_or_retract() should probably actually be validate_heads_or_retract(), since there can be multiple heads for the same arg. Internally we might construct an array 'head_ranges' that indicates the indicies in head_ptrs associated with each var. Then when we loop through the invalidated f_ids we can just extract 
what we need


--- Sept 15th ---

Need to determine what owns what... Do conditions link to a matcher instances or does the matcher instance control the conditions. It seems like the connection should go both ways. When c.get_matches(mem) is called then the matcher instance should be attached to c, and it should be accessible by c.matcher. One concern is that if the matcher can be of one of mutliple types then we may have a problem. For now maybe it makes sense to make RETE the only type of matcher.


--- Sept 17th ----
Had an idea to have a NodeMemoryEntry object that gets pooled and can clean itself up... so the output matches would look like u8 -> vector<*NodeMemoryEntry> The problem is that if a match gets invalidated we still need to delete it from the set which would require looping through... so it's probably not worth it.

I do still kind of like this idea of having entries in there that are just invalid,
I suppose since it is a u8->u1 anyway we could just make the u1 = 1 for valid and 0 for invalid. The advantage being that if something is retracted then declared again or backtracking occurs the output.matches would still have the same order which means a matches iterator wouldn't become invalid on a backtrack.


--- Sept 18th ----
Okay, so what needs to happen? 
[] setup the mem.facts->head_ptrs connection.  


--- Sept 21st ----
Thinking about inside the NodeOutput object there is .matches again
Realizing that the format of this depends a great deal on n_args, is there a way to do w/o having more than one nested dict?

There could be just one dict of (arg_ind, f_id)... but then it's useless for looping. Maybe this should actually be an AKD... of the match's f_ids

Could be two dictionaries, or a dictionary and a linked list or vector? The one dicitonary of (arg_ind, f_id)->NodeMemoryEntry is just there for cleanup, and the vector(s) are there for looping. Then they on retract the entries can just be invalidated instead of fully removed.

Needs to be able to:
	1) Clean up on the basis of retractions
	2) Clean up on the basis of failed matches
	3) Get a list of matches associated with each arg_ind and f_id

Dict(AKD[i8]->u1)


Alternatively match_inds[arg_ind][f_id] -> Dict(match_inds->u1)
matches[match_ind] = match

1+3 work... we can loop through matches via their match_inds
2 is harder

Dictionaries of Dictionaries may be the cleanest thing after_all,
although keeping a proper refcount might be challenging.



--- Sept 22 ---

It seems a bug is preventing me from keeping around iter(Dict()) instances so we are going to need to just make copies of everything such that they can be indexed directly:

For alpha end nodes an array will suffice
for beta end nodes the same array, but also another array that points to arrays

So a node iter might be like:
	graph : ReteGraphType,
	node: ReteNodeType,

	output : ReteNodeOutputType,
	downstream_ptr : i8,
	if(downstream_ptr)

	curr_ind : i8,

	var_inds: i8[::1],


Probably only need to copy as we go. Should do some preprocessing to get a structure that is relatively automatic.

Constraint, lets just have one MatchIterNode for each var. We need to loop backwards from the last end_node back to the first. Each MatchIterNode either depends on a downstream node or not. 

First construct IterNodes from downstream up... then on next() go from upstream down and iter each node. If a node can't increment then request an increment and update.


MatchIterator:


def init(var_end_nodes):
	iternodes = []

	# Init MatchIteratorNodes
	for i in range(n_vars-1,-1,-1):
		node = var_end_nodes[i]

		st = new(MatchIteratorNodeType)
		st.graph = self.graph
		st.node = node
		st.output = ...
		st.curr_ind = 0

	# Link MatchIteratorNodes
	for i in range(n_vars-1,-1,-1):
		node = 




def increment(self):
	node = self.iternodes[-1]

	self.curr_ind += 1
	if(self.curr_ind >= len(var_inds)):
		
		
--- Oct 12 ---

Organizing betas as a dict of dicts is a bit slow. In particular I suspect that initializing O(n) dicts is causing lag. It would probably suffice to just return to the way things were before where the truth value of betas is handled by a 2d array. This probably isn't the best data structure if I end up supporting 3-way or greater Ops, but I doubt that that is such a killer feature that it will become worth pursuing. 

Instead we can just keep idrec -> ind dictionaries, in the output... so the data structure is the same. Then on a retraction we can just del the idrec and assign -1 to the row/column. We probably also need to keep a vector of free indicies for each arg which would get push to on a retraction. On an insertion we need to expand the size of the array by whatever... 

There is a slight tension between the need to quickly index a truth value and the need to iterate over only the true ones. 

We need to be able to insert, retract, and iterate quickly. Insertion and retraction are quick for a truth table, but iteration is slower. 

Instantiating one big array neglibible compared to instantiating lost of little ones. 

Instantiating one dict that then gets big as also relatively faster. 

We need in any case a way to efficiently retrieve just those indicies that correspond to true items 


--- Oct 13 ---

So far using the truth table is considerably faster than dictionaries
Now it seems reasonable to cull out a few more uses of dictionaries. 
Here are all the dictionaries:
	idrec_to_ind : definitely needs to stay, this ensures that we can map arbirary idrecs into a fixed address space

	output.matches : might be able to do without... retraction could be achieved by zeroing the index found with idrec_to_ind

	output.change_set : probably can be replaced, in principle if this was just an array of idrecs then we could just slice it directly from the mem's change_queue

	arg_change_set : we could probably achieve the same thing by just keeping around a record array with an entry like .is_changed  

Okay so we can have something like "input_states" which is a list of record arrays with "idrec", "is_changed", "true_was_nonzero", "true_count" -> input_state_type. From this we can make  new arrays, maybe that share same buffers: "changed_idrecs", "unchanged_idrecs", "changed_inds" "unchanged_inds". We can use these to loop through pairs efficiently without overlap. The output.matches can be edited by looping through the changed_inputs. 

It is unclear if it is more efficient to leave holes in output.matches or to rebuild it each time. If holes are left then it seems we need vectors to keep track of the holes, like with the change queue, and probably a buffer in any case. The output.matches probably needs to be only the idrecs to be consistent with the alpha case and there should be an additional output.inds that can be used by the iterator. In the case of same parent and during iteration we would call get_other which would get the idrecs with nonzero elements along a row or column.
If "true_was_nonzero" and now "true_count" == 0, then retract 






---- Oct 28 ----
Predicates. Instead of having groundings it makes sense to just have predicates where the predicate header and all of its members are just some kind of generic type like CREObj which has some kind of type indicator that keeps track of which of [IntPrimitive, FloatPrimitive, UnicodePrimitive, Var, Op, Fact] it is. Maybe CREObjType designates idrec and BaseFact designates fact_num. When a predicate is declared....



----- Nov 28 ----
Should probably replace 'predicate' with 'wme' since officially a predicate is a function that can take a number of terms, but a wme is strictly gounded.

We need to be able to hash/eq a few tricky items
1) Var(): requires hashing 
	-'base_ptr'
	-the array of offsets
 possibilities:
	-instead of an array a fixed allocation at runtime
	-just hardcode it  
	-make a special hash function for the deref_type

2) Op(): requires hashing 
	-call, match addr, or name
	-the list of head_vars
 possibilities:
	-instead of an array a fixed allocation at runtime
	-just hardcode it  
	-make a special hash function for the List of head_vars

3) Literal: requires hashing
	-The underlying op
	-The negation 

4) Condition: requires hashing
	-All the literals + some extra thing for disjunction

if something like:

c1,c2 = (
	(x:=Var(float)) & 
	(y:=Var(float)) & (x < y)
)

Is instantiated twice then (c1 == c2) >>> true
even though x of c1 is not the same instance as c2

This makes me feel like we probably need to hardcode most of these. 
Since the hash of the literals won't be the same...

At the Op level however it might make sense to just disregard the particulars of the variables since Add(a,b) is functionally equivalent to Add(x,y)... or maybe not.

We would however not want (a < b) & (x > y) to be equal to (a < b) & (a > b) which would be the case if we disregarded the var instances. 

Perhaps the best thing then is to be as strict as possible with __hash__ and __eq__ for these and require that the vars are literally the same instance. This doesn't solve our above issue... but maybe that is fine, we could have an extra function like same_behavior() that checks for things like different variables, and reorderings of literals. 

   



